{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get text from webpages\n",
    "def gettext(url):\n",
    "    import requests\n",
    "    r=requests.get(url)\n",
    "    r.encoding = 'utf-8'\n",
    "    import bs4\n",
    "    mypage = bs4.BeautifulSoup(r.text)\n",
    "    text = mypage.find(attrs={'class': 'text_c'}).text.strip()\n",
    "    return text \n",
    "\n",
    "\n",
    "# append all urls \n",
    "urls = []\n",
    "date = []\n",
    "for c in range(1,4):\n",
    "    if c < 10:\n",
    "        m = str(c)\n",
    "        c = '0'+ m\n",
    "    else:\n",
    "        c = c\n",
    "    date.append(c)\n",
    "for c in date:\n",
    "    a = 1\n",
    "    while a < 10:\n",
    "        for b in range(1,5):\n",
    "            url = \"http://paper.people.com.cn/rmrb/html/2008-10/{2}/nw.D110000renmrb_200810{2}_{0}-0{1}.htm\".format(a,b,c)\n",
    "            urls.append(url)\n",
    "\n",
    "        a = a + 1\n",
    "        \n",
    "# collect text in tendays        \n",
    "tendays=[]\n",
    "for url in urls:\n",
    "    try:\n",
    "        text = gettext(url)\n",
    "        tendays.append(text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Error on page %s' % url)\n",
    "\n",
    "# write in text files        \n",
    "with open('2008三天.text', 'w') as f:\n",
    "    for item in tendays:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# counting word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutting text with jieba\n",
    "import jieba\n",
    "words = open('2018下半年.text',\"r\").read()\n",
    "jieba.add_word('一带一路')\n",
    "jieba.add_word('改革开放')\n",
    "jieba.add_word('人类命运共同体')\n",
    "jieba.add_word('大湾区')\n",
    "jieba.add_word('习近平新时代中国特色社会主义思想')\n",
    "jieba.add_word('十一届三中全会')\n",
    "jieba.add_word('三中全会')\n",
    "jieba.add_word('共享经济')\n",
    "jieba.add_word('习近平新时代中国特色社会主义')\n",
    "jieba.add_word('供给侧结构性改革')\n",
    "jieba.add_word('占中')\n",
    "jieba.add_word('天宫一号')\n",
    "jieba.add_word('上海世博会')\n",
    "jieba.add_word('嫦娥二号')\n",
    "jieba.add_word('科学发展观')\n",
    "jieba.add_word('两岸三通')\n",
    "jieba.add_word('问责')\n",
    "jieba.add_word('金融危机')\n",
    "jieba.add_word('限塑令')\n",
    "jieba.add_word('土地流转')\n",
    "jieba.add_word('三聚氰胺')\n",
    "words = jieba.cut(words)\n",
    "filepath = 'stopwords-zh.txt'\n",
    "stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]\n",
    "#remove stopwords\n",
    "processed_word_list = []\n",
    "for word in words:\n",
    "    if word not in stopwords and len(word)>1: #remove single word\n",
    "        processed_word_list.append(word)\n",
    "# counting word frequency\n",
    "wordcount={}\n",
    "for word in processed_word_list:\n",
    "    if word not in wordcount:\n",
    "        wordcount[word] = 1\n",
    "    else:\n",
    "        wordcount[word] += 1\n",
    "import operator\n",
    "sorted_wordcount = sorted(wordcount.items(), key=operator.itemgetter(1),reverse=True) \n",
    "\n",
    "# write wordfrequency csv file\n",
    "import csv\n",
    "with open('2018下半年.csv','w')as f:\n",
    "    mywriter=csv.writer(f)\n",
    "    mywriter.writerows(sorted_wordcount)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# draw wordcloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "!pip install Pillow\n",
    "from PIL import Image\n",
    "import wordcloud\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas\n",
    "df = pandas.read_csv('2008下半年.csv')\n",
    "df.columns=['keyword','frequency']\n",
    "df.set_index('keyword')\n",
    "# filter top 500 words\n",
    "word = df['keyword'][:500]\n",
    "\n",
    "filepath = 'stopwords-zh.txt'\n",
    "stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]\n",
    "def tag_cloud(text):\n",
    "    mask = np.array(Image.open('china outline.png')) #set mask, you can change to the picture you like, but it must have a high color contrast\n",
    "    wc = wordcloud.WordCloud(mode='RGBA',background_color='white',font_path='/Library/Fonts/Songti.ttc',max_words=2000,stopwords=stopwords,max_font_size=300,random_state=42,mask=mask)\n",
    "    wc.generate_from_text(' '.join(text))\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title('2008 Top 500', loc='Center', fontsize=30)\n",
    "    plt.show()\n",
    "    return plt.show()\n",
    "tag_cloud(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bar chart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "df = pd.DataFrame(dict(graph=['2008','2009', '2010', '2011','2012','2013','2014','2015','2016','2017','2018'],\n",
    "                           n=[42,25, 30, 24,0,86,24,121,36,139,91], m=[13,17, 32, 32, 0, 29,4,26,28,44,38])) \n",
    "\n",
    "ind = np.arange(len(df))\n",
    "width = 0.4\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(ind, df.n, width, color='red', label='President')\n",
    "ax.barh(ind + width, df.m, width, color='green', label='Prime Minister')\n",
    "\n",
    "ax.set(yticks=ind + width, yticklabels=df.graph, ylim=[2*width - 1, len(df)])\n",
    "ax.legend()\n",
    "plt.title('Frequency of Names in Three Days (10.1-10.3)')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud for country and city names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyecharts\n",
    "# 18年全年 国内地区\n",
    "from pyecharts import WordCloud\n",
    "word = ['北京','上海','海南','长江','香港','青岛','宁夏','雄安','福建','广东','重庆','深圳','四川','山东','武汉','河北','京津冀','浙江',\n",
    "       '厦门','江苏']\n",
    "values = [ 7007,2796, 1540,1478,  1144,    883,  702,   685,   639,   637,    619, 555,  543,   513,    495,  485,    480,    748,\n",
    "          445, 436]\n",
    "\n",
    "wordcloud = WordCloud(width=1300, height=620)\n",
    "wordcloud.add(\"\", word, values, word_size_range=[20, 100])\n",
    "wordcloud.render('region-in-renminribao.html')\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame('region-in-renminribao.html', width=700, height=620)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18年全年 国外地区\n",
    "from pyecharts import WordCloud\n",
    "words = ['非洲','美国','金砖','俄罗斯','南非','日本','东盟', '欧洲','泰国','巴基斯坦','德国','中东欧','塞内加尔','英国','印尼','亚洲',\n",
    "         '印度','欧盟','约翰内斯堡','阿联酋']\n",
    "value = [ 3515, 3923,  1396,   1031,     955,   782,  754,     576,   537,    527,     511,    508,     496,       493,   474, 1115\n",
    "        ,453,453,442,438]\n",
    "\n",
    "wordcloud = WordCloud(width=1300, height=620)\n",
    "wordcloud.add(\"\", words, value, word_size_range=[20, 100])\n",
    "wordcloud.render('trade-war-wordcloud.html')\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame('trade-war-wordcloud.html', width=700, height=620)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17年全年 国外地区\n",
    "from pyecharts import WordCloud\n",
    "word1 = ['美国','金砖','俄罗斯','非洲','东盟','越南','哈萨克斯坦','欧洲' ,'巴西','老挝',\n",
    "        '亚洲','日本','印度','巴基斯坦','瑞士',  '泰国','英国','欧盟','芬兰','韩国']\n",
    "value1 = [  2440,  3891,  1388,    1343, 1206,  1036,     943,     877  ,   862    ,822,  \n",
    "        808,799,        648,    646,   607,     594,    587,    572,   562,475]\n",
    "\n",
    "wordcloud = WordCloud(width=1300, height=620)\n",
    "wordcloud.add(\"\", word1, value1, word_size_range=[20, 100])\n",
    "wordcloud.render('2017-foreign-countries.html')\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame('2017-foreign-countries.html', width=700, height=620)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17年全年 国内地区\n",
    "from pyecharts import WordCloud\n",
    "word2 = ['北京','香港','上海','河北','厦门','内蒙古','浙江','雄安' ,'天津', '杭州' ,\n",
    "        '新疆' ,'长江','四川','广西','广东',   '西藏','辽宁','福建','南海','山东','贵州']\n",
    "value2 = [7980,   2451,1985,  1035,  906 ,890,    819,      737   ,700 ,     632,   \n",
    "          623, 566 , 559 , 533,      494 ,     484,   483,     459, 459,452,450]\n",
    "\n",
    "wordcloud = WordCloud(width=1300, height=620)\n",
    "wordcloud.add(\"\", word2, value2, word_size_range=[20, 100])\n",
    "wordcloud.render('2017-cities-provinces.html')\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame('2017-cities-provinces.html', width=700, height=620)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08年下半年 国内地区\n",
    "from pyecharts import WordCloud\n",
    "word3 = ['北京','上海','四川','广西','宁夏','汶川','台湾','西藏','香港','天津',\n",
    "        '广东','浙江','成都','广州','江苏','南宁','内蒙古','青岛','重庆','福建']\n",
    "value3 = [9484,1176,1026, 836,        832 ,810,      604,   591,538,493,\n",
    "        289 ,263,247           ,245    ,234,214,207,201,194,191]\n",
    "\n",
    "wordcloud = WordCloud(width=1300, height=620)\n",
    "wordcloud.add(\"\", word3, value3, word_size_range=[20, 100])\n",
    "wordcloud.render('2008-cities-provinces.html')\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame('2008-cities-provinces.html', width=700, height=620)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18年下半年 国内地区\n",
    "from pyecharts import WordCloud\n",
    "word5 = ['北京','上海','香港','宁夏','海南','广西','浙江','重庆','广东','天津',\n",
    "         '四川','澳门','武汉','山东','江苏','大湾','粤港澳','浦东','福建','南海']\n",
    "value5 = [3696,1459, 768 ,558, 357 ,341 ,337                ,337    ,335,259,     \n",
    "          253,246,241,240,236,                234,227 ,216      ,215,208]\n",
    "\n",
    "wordcloud = WordCloud(width=1300, height=620)\n",
    "wordcloud.add(\"\", word5, value5, word_size_range=[20, 100])\n",
    "wordcloud.render('2018-cities-provinces.html')\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame('2018-cities-provinces.html', width=700, height=620)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18年下半年 国外地区\n",
    "from pyecharts import WordCloud\n",
    "word6 = ['非洲','美国','金砖','南非','俄罗斯','东盟','塞内加尔','卢旺达','中东欧','阿联酋',\n",
    "         '约翰内斯堡','日本','西班牙','新加坡','欧洲','菲律宾','亚洲','欧盟','文莱','阿根廷']\n",
    "value6 = [ 2977, 2124, 1296,866,        496,   772,  480 ,478,431,                 431,\n",
    "         401,          397,     392,    372,     363,   362,   357,  326,315,270]\n",
    "\n",
    "wordcloud = WordCloud(width=1300, height=620)\n",
    "wordcloud.add(\"\", word6, value6, word_size_range=[20, 100])\n",
    "wordcloud.render('2018-foreign-countries.html')\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame('2018-foreign-countries.html', width=700, height=620)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
